{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/RandomForest_model.joblib...\n",
      "RandomForest Accuracy: 0.7990124359912216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89      4329\n",
      "           1       0.70      0.06      0.11      1139\n",
      "\n",
      "    accuracy                           0.80      5468\n",
      "   macro avg       0.75      0.53      0.50      5468\n",
      "weighted avg       0.78      0.80      0.73      5468\n",
      "\n",
      "Model GradientBoosting already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/GradientBoosting_model.joblib...\n",
      "GradientBoosting Accuracy: 0.7991953182150695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89      4329\n",
      "           1       0.70      0.06      0.11      1139\n",
      "\n",
      "    accuracy                           0.80      5468\n",
      "   macro avg       0.75      0.53      0.50      5468\n",
      "weighted avg       0.78      0.80      0.73      5468\n",
      "\n",
      "Model XGBoost already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/XGBoost_model.joblib...\n",
      "XGBoost Accuracy: 0.7990124359912216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89      4329\n",
      "           1       0.70      0.06      0.11      1139\n",
      "\n",
      "    accuracy                           0.80      5468\n",
      "   macro avg       0.75      0.53      0.50      5468\n",
      "weighted avg       0.78      0.80      0.73      5468\n",
      "\n",
      "Model LightGBM already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/LightGBM_model.joblib...\n",
      "LightGBM Accuracy: 0.7991953182150695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89      4329\n",
      "           1       0.70      0.06      0.11      1139\n",
      "\n",
      "    accuracy                           0.80      5468\n",
      "   macro avg       0.75      0.53      0.50      5468\n",
      "weighted avg       0.78      0.80      0.73      5468\n",
      "\n",
      "Model CatBoost already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/CatBoost_model.joblib...\n",
      "CatBoost Accuracy: 0.7993782004389174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89      4329\n",
      "           1       0.71      0.06      0.12      1139\n",
      "\n",
      "    accuracy                           0.80      5468\n",
      "   macro avg       0.75      0.53      0.50      5468\n",
      "weighted avg       0.78      0.80      0.73      5468\n",
      "\n",
      "Model SVM already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/SVM_model.joblib...\n",
      "SVM Accuracy: 0.7920629114850036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      4329\n",
      "           1       0.62      0.00      0.01      1139\n",
      "\n",
      "    accuracy                           0.79      5468\n",
      "   macro avg       0.71      0.50      0.45      5468\n",
      "weighted avg       0.76      0.79      0.70      5468\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 4512, number of negative: 17357\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 21869, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.206319 -> initscore=-1.347255\n",
      "[LightGBM] [Info] Start training from score -1.347255\n",
      "Transaktionsgebühren vor der Prognose: 73291.5\n",
      "Transaktionsgebühren nach der Prognose: 16934\n",
      "Transaktionsgebühren nach der Prognose für RandomForest: 16937\n",
      "Transaktionsgebühren nach der Prognose für GradientBoosting: 16934\n",
      "Transaktionsgebühren nach der Prognose für XGBoost: 16931\n",
      "Transaktionsgebühren nach der Prognose für LightGBM: 16934\n",
      "Transaktionsgebühren nach der Prognose für CatBoost: 16931\n",
      "Transaktionsgebühren nach der Prognose für SVM: 17213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import uniform, randint\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Daten einlesen (Beispieldaten, bitte durch Ihre Daten ersetzen)\n",
    "model_dir = '/Users/userlow/Documents/kreditkarte_aufgabe/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "file_path = '/Users/userlow/Documents/kreditkarte_aufgabe/data/bereinigte_daten.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Umwandeln von kategorischen Merkmalen in numerische\n",
    "data['country'] = data['country'].astype('category').cat.codes\n",
    "data['PSP'] = data['PSP'].astype('category').cat.codes\n",
    "data['card'] = data['card'].astype('category').cat.codes\n",
    "\n",
    "# Features und Zielvariable definieren\n",
    "X = data[['amount', 'country', 'PSP', '3D_secured', 'card']]\n",
    "y = data['success']\n",
    "\n",
    "# Datenaufteilung in Trainings- und Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelle definieren\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Angepasste Hyperparameter für RandomizedSearchCV\n",
    "param_grids = {\n",
    "    'RandomForest': [\n",
    "        {   # Suchraum 1: Mit Bootstrap und OOB-Scores\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': [None] + list(range(10, 51)),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'bootstrap': [True],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "            'max_leaf_nodes': [None] + list(range(10, 51)),\n",
    "            'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "            'oob_score': [True],\n",
    "            'warm_start': [False, True],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "        },\n",
    "        {   # Suchraum 2: Ohne OOB-Scores\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': [None] + list(range(10, 51)),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'bootstrap': [False],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "            'max_leaf_nodes': [None] + list(range(10, 51)),\n",
    "            'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "            'oob_score': [False],\n",
    "            'warm_start': [False, True],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "        }\n",
    "    ],\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.8, 0.2),\n",
    "        'colsample_bytree': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'num_leaves': randint(31, 127),\n",
    "        'subsample': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'depth': randint(3, 10)\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': uniform(0.1, 10),\n",
    "        'gamma': uniform(0.01, 1),\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Modelle trainieren, bewerten und speichern\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    model_path = os.path.join(model_dir, f\"{model_name}_model.joblib\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model {model_name} already exists. Loading the model from {model_path}...\")\n",
    "        best_models[model_name] = joblib.load(model_path)\n",
    "    else:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        if model_name == 'RandomForest':\n",
    "            # RandomForest separat behandeln\n",
    "            best_estimator = None\n",
    "            best_score = -np.inf\n",
    "            for param_grid in param_grids['RandomForest']:\n",
    "                random_search = RandomizedSearchCV(model, param_grid, n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')\n",
    "                random_search.fit(X_train, y_train)\n",
    "                if random_search.best_score_ > best_score:\n",
    "                    best_score = random_search.best_score_\n",
    "                    best_estimator = random_search.best_estimator_\n",
    "            best_models[model_name] = best_estimator\n",
    "        else:\n",
    "            random_search = RandomizedSearchCV(model, param_grids[model_name], n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')\n",
    "            random_search.fit(X_train, y_train)\n",
    "            best_models[model_name] = random_search.best_estimator_\n",
    "        \n",
    "        # Speichern des Modells\n",
    "        joblib.dump(best_models[model_name], model_path)\n",
    "        print(f\"Model {model_name} saved at {model_path}\")\n",
    "    \n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Überprüfen der gespeicherten Modelle\n",
    "for model_name in best_models.keys():\n",
    "    model_path = os.path.join(model_dir, f\"{model_name}_model.joblib\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model {model_name} was not saved properly!\")\n",
    "\n",
    "# Annahmen für die Transaktionsgebühren\n",
    "transaction_fees = {\n",
    "    0: {'success': 2, 'failure': 5},  # PSP 0\n",
    "    1: {'success': 1, 'failure': 3},  # PSP 1\n",
    "    2: {'success': 0.5, 'failure': 1},  # PSP 2\n",
    "    3: {'success': 1.5, 'failure': 4}   # PSP 3\n",
    "}\n",
    "\n",
    "# Gebührenberechnung\n",
    "def calculate_fees(df, success_col):\n",
    "    fees = 0\n",
    "    for index, row in df.iterrows():\n",
    "        psp = row['PSP']\n",
    "        success = row[success_col]\n",
    "        fees += transaction_fees[psp]['success'] if success == 1 else transaction_fees[psp]['failure']\n",
    "    return fees\n",
    "\n",
    "fees_before = calculate_fees(data, 'success')\n",
    "\n",
    "# Ensemble-Modell erstellen\n",
    "estimators = [(name, model) for name, model in best_models.items()]\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "X_test_with_actual = X_test.copy()\n",
    "X_test_with_actual['success'] = y_test.values\n",
    "X_test_with_actual['predicted_success'] = y_pred\n",
    "\n",
    "fees_after = calculate_fees(X_test_with_actual, 'predicted_success')\n",
    "\n",
    "print(f\"Transaktionsgebühren vor der Prognose: {fees_before}\")\n",
    "print(f\"Transaktionsgebühren nach der Prognose: {fees_after}\")\n",
    "\n",
    "# Berechnung der Gebühren nach der Prognose für jedes Modell\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    X_test_with_actual = X_test.copy()\n",
    "    X_test_with_actual['success'] = y_test.values\n",
    "    X_test_with_actual['predicted_success'] = y_pred\n",
    "\n",
    "    fees_after = calculate_fees(X_test_with_actual, 'predicted_success')\n",
    "    print(f\"Transaktionsgebühren nach der Prognose für {model_name}: {fees_after}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
