{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/RandomForest_model.joblib...\n",
      "RandomForest Accuracy: 0.8127355683396151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      8162\n",
      "           1       0.60      0.05      0.09      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.71      0.52      0.49     10082\n",
      "weighted avg       0.77      0.81      0.74     10082\n",
      "\n",
      "Model GradientBoosting already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/GradientBoosting_model.joblib...\n",
      "GradientBoosting Accuracy: 0.8127355683396151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      8162\n",
      "           1       0.60      0.05      0.09      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.71      0.52      0.49     10082\n",
      "weighted avg       0.77      0.81      0.74     10082\n",
      "\n",
      "Model XGBoost already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/XGBoost_model.joblib...\n",
      "XGBoost Accuracy: 0.8127355683396151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      8162\n",
      "           1       0.60      0.05      0.09      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.71      0.52      0.49     10082\n",
      "weighted avg       0.77      0.81      0.74     10082\n",
      "\n",
      "Model LightGBM already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/LightGBM_model.joblib...\n",
      "LightGBM Accuracy: 0.8127355683396151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      8162\n",
      "           1       0.60      0.05      0.09      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.71      0.52      0.49     10082\n",
      "weighted avg       0.77      0.81      0.74     10082\n",
      "\n",
      "Model CatBoost already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/CatBoost_model.joblib...\n",
      "CatBoost Accuracy: 0.8128347550089268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      8162\n",
      "           1       0.60      0.05      0.09      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.71      0.52      0.49     10082\n",
      "weighted avg       0.78      0.81      0.74     10082\n",
      "\n",
      "Model SVM already exists. Loading the model from /Users/userlow/Documents/kreditkarte_aufgabe/models/SVM_model.joblib...\n",
      "SVM Accuracy: 0.8104542749454473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90      8162\n",
      "           1       0.76      0.01      0.01      1920\n",
      "\n",
      "    accuracy                           0.81     10082\n",
      "   macro avg       0.79      0.50      0.45     10082\n",
      "weighted avg       0.80      0.81      0.73     10082\n",
      "\n",
      "Loading Basis Model from /Users/userlow/Documents/kreditkarte_aufgabe/models/basis_model.joblib...\n",
      "[LightGBM] [Info] Number of positive: 8308, number of negative: 32020\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.206011 -> initscore=-1.349142\n",
      "[LightGBM] [Info] Start training from score -1.349142\n",
      "Transaktionsgebühren vor der Prognose: 137835.0\n",
      "Transaktionsgebühren nach der Prognose für Ensemble Modell: 31538\n",
      "Transaktionsgebühren nach der Prognose für RandomForest: 31304\n",
      "Transaktionsgebühren nach der Prognose für GradientBoosting: 31298\n",
      "Transaktionsgebühren nach der Prognose für XGBoost: 31292\n",
      "Transaktionsgebühren nach der Prognose für LightGBM: 31298\n",
      "Transaktionsgebühren nach der Prognose für CatBoost: 31295\n",
      "Transaktionsgebühren nach der Prognose für SVM: 31733\n",
      "Transaktionsgebühren nach der Prognose für BasisModel: 29113.0\n",
      "[LightGBM] [Info] Number of positive: 8183, number of negative: 32145\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202911 -> initscore=-1.368198\n",
      "[LightGBM] [Info] Start training from score -1.368198\n",
      "[LightGBM] [Info] Number of positive: 8183, number of negative: 32145\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202911 -> initscore=-1.368198\n",
      "[LightGBM] [Info] Start training from score -1.368198\n",
      "[LightGBM] [Info] Number of positive: 8182, number of negative: 32146\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202886 -> initscore=-1.368351\n",
      "[LightGBM] [Info] Start training from score -1.368351\n",
      "[LightGBM] [Info] Number of positive: 8182, number of negative: 32146\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202886 -> initscore=-1.368351\n",
      "[LightGBM] [Info] Start training from score -1.368351\n",
      "[LightGBM] [Info] Number of positive: 8182, number of negative: 32146\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 267\n",
      "[LightGBM] [Info] Number of data points in the train set: 40328, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.202886 -> initscore=-1.368351\n",
      "[LightGBM] [Info] Start training from score -1.368351\n",
      "              Model  Average Accuracy  Std Accuracy\n",
      "0      RandomForest          0.798929      0.001312\n",
      "1  GradientBoosting          0.798869      0.001133\n",
      "2           XGBoost          0.799464      0.001752\n",
      "3          LightGBM          0.799643      0.001721\n",
      "4          CatBoost          0.799484      0.001727\n",
      "5               SVM          0.796985      0.000171\n",
      "6        BasisModel          0.737334      0.006458\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import uniform, randint\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Daten einlesen (Beispieldaten, bitte durch Ihre Daten ersetzen)\n",
    "model_dir = '/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "file_path = '/data/PSP_Jan_Feb_2019.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Umwandeln von kategorischen Merkmalen in numerische\n",
    "data['country'] = data['country'].astype('category').cat.codes\n",
    "data['PSP'] = data['PSP'].astype('category').cat.codes\n",
    "data['card'] = data['card'].astype('category').cat.codes\n",
    "\n",
    "# Features und Zielvariable definieren\n",
    "X = data[['amount', 'country', 'PSP', '3D_secured', 'card']]\n",
    "y = data['success']\n",
    "\n",
    "# Datenaufteilung in Trainings- und Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelle definieren\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Angepasste Hyperparameter für RandomizedSearchCV\n",
    "param_grids = {\n",
    "    'RandomForest': [\n",
    "        {   # Suchraum 1: Mit Bootstrap und OOB-Scores\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': [None] + list(range(10, 51)),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'bootstrap': [True],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "            'max_leaf_nodes': [None] + list(range(10, 51)),\n",
    "            'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "            'oob_score': [True],\n",
    "            'warm_start': [False, True],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "        },\n",
    "        {   # Suchraum 2: Ohne OOB-Scores\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': [None] + list(range(10, 51)),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'bootstrap': [False],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "            'max_leaf_nodes': [None] + list(range(10, 51)),\n",
    "            'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "            'oob_score': [False],\n",
    "            'warm_start': [False, True],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "        }\n",
    "    ],\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'subsample': uniform(0.8, 0.2),\n",
    "        'colsample_bytree': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'num_leaves': randint(31, 127),\n",
    "        'subsample': uniform(0.8, 0.2)\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': randint(100, 500),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'depth': randint(3, 10)\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': uniform(0.1, 10),\n",
    "        'gamma': uniform(0.01, 1),\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Modelle trainieren, bewerten und speichern\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    model_path = os.path.join(model_dir, f\"{model_name}_model.joblib\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model {model_name} already exists. Loading the model from {model_path}...\")\n",
    "        best_models[model_name] = joblib.load(model_path)\n",
    "    else:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        if model_name == 'RandomForest':\n",
    "            # RandomForest separat behandeln\n",
    "            best_estimator = None\n",
    "            best_score = -np.inf\n",
    "            for param_grid in param_grids['RandomForest']:\n",
    "                random_search = RandomizedSearchCV(model, param_grid, n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')\n",
    "                random_search.fit(X_train, y_train)\n",
    "                if random_search.best_score_ > best_score:\n",
    "                    best_score = random_search.best_score_\n",
    "                    best_estimator = random_search.best_estimator_\n",
    "            best_models[model_name] = best_estimator\n",
    "        else:\n",
    "            random_search = RandomizedSearchCV(model, param_grids[model_name], n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')\n",
    "            random_search.fit(X_train, y_train)\n",
    "            best_models[model_name] = random_search.best_estimator_\n",
    "        \n",
    "        # Speichern des Modells\n",
    "        joblib.dump(best_models[model_name], model_path)\n",
    "        print(f\"Model {model_name} saved at {model_path}\")\n",
    "    \n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Überprüfen der gespeicherten Modelle\n",
    "for model_name in best_models.keys():\n",
    "    model_path = os.path.join(model_dir, f\"{model_name}_model.joblib\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model {model_name} was not saved properly!\")\n",
    "# Laden des Basis-Modells\n",
    "basis_model_path = '/models/basis_model.joblib'\n",
    "if os.path.exists(basis_model_path):\n",
    "    print(f\"Loading Basis Model from {basis_model_path}...\")\n",
    "    best_models['BasisModel'] = joblib.load(basis_model_path)\n",
    "else:\n",
    "    print(\"Warning: Basis Model not found!\")\n",
    "\n",
    "# Annahmen für die Transaktionsgebühren\n",
    "transaction_fees = {\n",
    "    0: {'success': 2, 'failure': 5},  # PSP 0\n",
    "    1: {'success': 1, 'failure': 3},  # PSP 1\n",
    "    2: {'success': 0.5, 'failure': 1},  # PSP 2\n",
    "    3: {'success': 1.5, 'failure': 4}   # PSP 3\n",
    "}\n",
    "\n",
    "# Gebührenberechnung\n",
    "def calculate_fees(df, success_col):\n",
    "    fees = 0\n",
    "    for index, row in df.iterrows():\n",
    "        psp = row['PSP']\n",
    "        success = row[success_col]\n",
    "        fees += transaction_fees[psp]['success'] if success == 1 else transaction_fees[psp]['failure']\n",
    "    return fees\n",
    "\n",
    "fees_before = calculate_fees(data, 'success')\n",
    "\n",
    "# Ensemble-Modell erstellen\n",
    "estimators = [(name, model) for name, model in best_models.items()]\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "X_test_with_actual = X_test.copy()\n",
    "X_test_with_actual['success'] = y_test.values\n",
    "X_test_with_actual['predicted_success'] = y_pred\n",
    "\n",
    "fees_after = calculate_fees(X_test_with_actual, 'predicted_success')\n",
    "\n",
    "print(f\"Transaktionsgebühren vor der Prognose: {fees_before}\")\n",
    "print(f\"Transaktionsgebühren nach der Prognose für Ensemble Modell: {fees_after}\")\n",
    "\n",
    "\n",
    "# Berechnung der Gebühren nach der Prognose für jedes Modell, einschließlich BasisModel\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    X_test_with_actual = X_test.copy()\n",
    "    X_test_with_actual['success'] = y_test.values\n",
    "    X_test_with_actual['predicted_success'] = y_pred\n",
    "\n",
    "    fees_after = calculate_fees(X_test_with_actual, 'predicted_success')\n",
    "    print(f\"Transaktionsgebühren nach der Prognose für {model_name}: {fees_after}\")\n",
    "\n",
    "# Sweet Spot Analysis\n",
    "def sweet_spot_analysis(models, X, y):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "        avg_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        results.append((model_name, avg_score, std_score))\n",
    "    results_df = pd.DataFrame(results, columns=['Model', 'Average Accuracy', 'Std Accuracy'])\n",
    "    return results_df\n",
    "\n",
    "# Ergebnisse der Sweet Spot Analysis anzeigen\n",
    "sweet_spot_results = sweet_spot_analysis(best_models, X, y)\n",
    "print(sweet_spot_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
